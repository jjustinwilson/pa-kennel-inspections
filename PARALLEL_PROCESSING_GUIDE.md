# ðŸš€ Parallel Processing Guide - 3-5x Faster Imports!

## What's New

The PDF import system now supports **parallel processing** using multiple CPU cores, dramatically speeding up the import process!

## Speed Comparison

| Workers | Time for 46,706 PDFs | Speedup |
|---------|---------------------|---------|
| 1 (original) | ~13 hours | 1x |
| 4 workers | ~4 hours | 3x faster |
| 8 workers | ~2-3 hours | 4-5x faster |

## Quick Start - Parallel Import

### Option 1: Use the Batch Runner (Recommended)

The batch runner now uses 8 workers by default:

```bash
cd /Users/jjustinwilson/Desktop/kennel
./run_batch_import.sh
```

This will automatically process all PDFs with 8 parallel workers!

**Custom worker count:**
```bash
# Use 4 workers
./run_batch_import.sh 0 0 4

# Resume from 5000 with 6 workers
./run_batch_import.sh 5000 0 6
```

### Option 2: Direct Python Command

```bash
cd /Users/jjustinwilson/Desktop/kennel
source venv/bin/activate

# Process with 8 workers (recommended)
python import_pdfs.py --workers 8

# Process specific batch with 8 workers
python import_pdfs.py --start 0 --end 5000 --workers 8

# Continue next batch
python import_pdfs.py --start 5000 --end 10000 --no-schema --workers 8
```

## How It Works

### Parallel PDF Parsing
- Multiple CPU cores parse PDFs simultaneously
- Each worker processes different PDFs in parallel
- Database writes remain sequential (SQLite requirement)

### Architecture
```
Main Process
    â”œâ”€ Worker 1 â†’ Parse PDF 1, 5, 9, 13...
    â”œâ”€ Worker 2 â†’ Parse PDF 2, 6, 10, 14...
    â”œâ”€ Worker 3 â†’ Parse PDF 3, 7, 11, 15...
    â””â”€ Worker 4 â†’ Parse PDF 4, 8, 12, 16...
           â†“
    Sequential DB writes (thread-safe)
```

## Choosing Worker Count

### Recommended Settings

**For MacBook Pro / Desktop:**
- **8 workers** - Best balance of speed and system responsiveness
- Use: `--workers 8`

**For Laptop / Older Mac:**
- **4 workers** - Good speed, less CPU load
- Use: `--workers 4`

**For Server / High-end Machine:**
- **12-16 workers** - Maximum speed
- Use: `--workers 12`

### How to Check Your CPU Cores

```bash
# macOS
sysctl -n hw.ncpu

# Linux
nproc
```

**Rule of thumb:** Use 75% of your CPU cores (e.g., 8 cores â†’ use 6 workers)

## Performance Tips

### 1. Don't Use Too Many Workers
- More workers â‰  always faster
- Sweet spot: 4-8 workers
- Beyond 8-10 workers, diminishing returns

### 2. Monitor System Resources
```bash
# Check CPU usage while running
top -pid $(pgrep -f import_pdfs.py)
```

### 3. Batch Size Recommendations

With parallel processing:
- **Batch size: 2000-5000** PDFs per batch
- Larger batches are fine with parallel processing
- Example:
  ```bash
  python import_pdfs.py --start 0 --end 5000 --workers 8
  python import_pdfs.py --start 5000 --end 10000 --no-schema --workers 8
  ```

## Examples

### Complete Import with 8 Workers
```bash
cd /Users/jjustinwilson/Desktop/kennel
source venv/bin/activate

# Process all 46,706 PDFs with 8 workers (~2-3 hours)
python import_pdfs.py --workers 8
```

### Batch Processing with Parallel Workers
```bash
# First 10,000 PDFs
python import_pdfs.py --start 0 --end 10000 --workers 8

# Next 10,000
python import_pdfs.py --start 10000 --end 20000 --no-schema --workers 8

# Continue...
python import_pdfs.py --start 20000 --end 30000 --no-schema --workers 8
```

### Resume After Interruption
```bash
# Check progress
python check_progress.py

# Resume from where you left off (e.g., 15,000)
python import_pdfs.py --start 15000 --no-schema --workers 8
```

## Troubleshooting

### "Too many open files" Error
Reduce worker count:
```bash
python import_pdfs.py --workers 4
```

### System Slowing Down
Use fewer workers:
```bash
python import_pdfs.py --workers 2
```

### Database Locked Errors
- This shouldn't happen (writes are sequential)
- If it does, reduce workers or add delay between batches

### Memory Issues
- Parallel processing uses more memory
- Reduce workers: `--workers 4`
- Or process smaller batches: `--end 2000`

## Comparing Performance

### Test Run (50 PDFs)
```bash
# Sequential (1 worker)
time python import_pdfs.py --start 0 --end 50
# Result: ~60 seconds

# Parallel (4 workers)
time python import_pdfs.py --start 0 --end 50 --workers 4
# Result: ~20 seconds (3x faster!)
```

## Current Status

You currently have **~1,000 PDFs** imported out of 46,706.

### To Complete Import Quickly:

**Option A: Automated (Recommended)**
```bash
# Will complete in ~2-3 hours with 8 workers
./run_batch_import.sh 1000
```

**Option B: Manual Control**
```bash
# Resume from where current import left off
python import_pdfs.py --start 1000 --no-schema --workers 8
```

## Why This Is Fast

1. **CPU Parallelization**: PDF text extraction uses multiple cores
2. **I/O Optimization**: While one worker waits for disk, others process
3. **Efficient Parsing**: Regex parsing distributed across workers
4. **Smart Batching**: Database writes grouped efficiently

## Summary

- âœ… **3-5x faster** than sequential processing
- âœ… **No GPU needed** - uses CPU cores efficiently
- âœ… **Same accuracy** - all data extracted correctly
- âœ… **Easy to use** - just add `--workers 8`
- âœ… **Resumable** - can stop/start anytime

**Bottom line:** Your 13-hour import is now a **2-3 hour import** with `--workers 8`! ðŸŽ‰
